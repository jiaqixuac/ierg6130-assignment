# IERG 6130 Assignment 4 Report

Student Name: Jiaqi XU

Studnet ID: 1155114887

## Winning Rate Matrix

|          |   MY_AGENT |   RANDOM |   WEAK |   MEDIUM |   STRONG |   RULE_BASED |   ALPHA_PONG |
|:---------|-----------:|---------:|-------:|---------:|---------:|-------------:|-------------:|
| MY_AGENT |       0.49 |        1 |      1 |        1 |        1 |         0.96 |         0.98 |

## Reward Matrix

|          |   MY_AGENT |   RANDOM |   WEAK |   MEDIUM |   STRONG |   RULE_BASED |   ALPHA_PONG |
|:---------|-----------:|---------:|-------:|---------:|---------:|-------------:|-------------:|
| MY_AGENT |      -0.26 |     18.4 |   9.66 |     9.98 |    10.14 |         8.66 |         9.36 |

## Description (if applicable)

Pure RL in the model-free setting.

In this assignment, I didn't change network structure or algorithm, while I only want to see the effect of opponent selecting strategies.

Mainly into three rounds of training:

	1. train with default CompetitivePong-v0 and CompetitivePongTournament-v0

	2. finetune using "1st" to beat [Rule_Based, Alpha_Pong, 1st round]

	3. finetune using "2nd" and self-play to beat [Rule_Based, Alpha_Pong, 2nd round]

Train for several iterations for the 3rd round

## Finding

Interesting findings:

	1. If B beats A, C beats B, then cannot show C can confidently beat A.

	2. Given a policy, we can easily train a policy to beat this policy using the same network structure.

	3. Through visual inspect, I find the trained policy can partially bias to its opponent rather than learn general strategy such as always moving to the hitting points of the ball.

I believe model-based methods will beat model-free methods such as explicitly adding the prediction of ball movements.

## Notes

Note: I change some parts of train.py, it's not totally compatible with the original code.

The best local results are from 2nd round in terms of Reward Matrix. I submit the checkpoint by local checkpoint competitions.

## Winning Rate Matrix

|          |   MY_AGENT |   RANDOM |   WEAK |   MEDIUM |   STRONG |   RULE_BASED |   ALPHA_PONG |
|:---------|-----------:|---------:|-------:|---------:|---------:|-------------:|-------------:|
| MY_AGENT |       0.48 |        1 |      1 |        1 |        1 |         0.99 |         0.99 |

## Reward Matrix

|          |   MY_AGENT |   RANDOM |   WEAK |   MEDIUM |   STRONG |   RULE_BASED |   ALPHA_PONG |
|:---------|-----------:|---------:|-------:|---------:|---------:|-------------:|-------------:|
| MY_AGENT |       0.28 |    17.76 |  12.78 |    13.64 |    11.88 |         8.68 |        11.48 |
